# -*- coding: utf-8 -*-
import os
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForCausalLM

class Config:
    model_path = "distilgpt2"  # å°æ¨¡å‹èŠ‚çœæ˜¾å­˜
    epochs = 1
    learning_rate = 2e-5
    max_seq_len = 64
    batch_size = 2
    device = "cuda" if torch.cuda.is_available() else "cpu"

config = Config()

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # é•œåƒåŠ é€Ÿ

class DummyDataset(Dataset):
    def __init__(self, tokenizer, max_seq_len):
        self.samples = [
            "I love this movie!",
            "I hate this movie!",
            "It was an average experience."
        ]
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.tokenizer(
            self.samples[idx],
            truncation=True,
            max_length=self.max_seq_len,
            padding="max_length",
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k, v in item.items()}
        item["labels"] = item["input_ids"].clone()
        return item

def train():
    # --------------------------
    # 3ï¸âƒ£ åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
    # --------------------------
    print("ğŸ§  åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨...")

    tokenizer = AutoTokenizer.from_pretrained(
        config.model_path,
        trust_remote_code=True
    )

    # âœ… ä¿®å¤ pad_token é—®é¢˜
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        config.model_path,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    ).to(config.device)

    dataset = DummyDataset(tokenizer, config.max_seq_len)
    train_loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)

    optimizer = AdamW(model.parameters(), lr=config.learning_rate)

    print("ğŸ§  å¼€å§‹è®­ç»ƒ...")
    model.train()
    for epoch in range(config.epochs):
        total_loss = 0
        for batch in train_loader:
            batch = {k: v.to(config.device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            total_loss += loss.item()

        print(f"ğŸ“Š Epoch {epoch + 1} | å¹³å‡Loss: {total_loss / len(train_loader):.4f}")

    os.makedirs("./trained_model", exist_ok=True)
    torch.save(model.state_dict(), "./trained_model/model.pt")
    print("âœ… æ¨¡å‹å·²ä¿å­˜è‡³ï¼š./trained_model/model.pt")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    train()
    print("âœ… è®­ç»ƒç»“æŸï¼Œæ¨¡å‹å·²ä¿å­˜ï¼")
