import json
import csv
import re
from sklearn.metrics import f1_score

def normalize_text(text):
    """ç»Ÿä¸€æ–‡æœ¬æ ¼å¼ï¼šå°å†™ã€å»é™¤æ ‡ç‚¹ã€å¤šä½™ç©ºæ ¼"""
    text = text.lower().strip()
    text = re.sub(r"[^a-z0-9\u4e00-\u9fff]+", " ", text)  # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
    text = re.sub(r"\s+", " ", text)
    return text

def compute_f1(prediction, ground_truth):
    """åŸºäºè¯é‡å çš„ F1"""
    pred_tokens = normalize_text(prediction).split()
    gt_tokens = normalize_text(ground_truth).split()
    if not pred_tokens or not gt_tokens:
        return 0.0

    common = set(pred_tokens) & set(gt_tokens)
    if len(common) == 0:
        return 0.0

    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(gt_tokens)
    if precision + recall == 0:
        return 0.0
    return 2 * precision * recall / (precision + recall)

def compute_exact_match(prediction, ground_truth):
    """åˆ¤æ–­æ˜¯å¦ç²¾ç¡®åŒ¹é…ï¼ˆæ ‡å‡†åŒ–åå®Œå…¨ä¸€è‡´ï¼‰"""
    return int(normalize_text(prediction) == normalize_text(ground_truth))

def evaluate_jsonl(file_path, output_csv="evaluation_results.csv"):
    results = []
    f1_scores = []
    em_scores = []

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            pred = data["prediction"]
            gt = data["ground_truth"]
            f1 = compute_f1(pred, gt)
            em = compute_exact_match(pred, gt)

            f1_scores.append(f1)
            em_scores.append(em)
            results.append({
                "id": data.get("id", ""),
                "prediction": pred,
                "ground_truth": gt,
                "F1": round(f1, 4),
                "Exact_Match": em
            })

    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open(output_csv, "w", newline="", encoding="utf-8-sig") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=results[0].keys())
        writer.writeheader()
        writer.writerows(results)

    avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0
    avg_em = sum(em_scores) / len(em_scores) if em_scores else 0

    print("âœ… Evaluation completed")
    print(f"ğŸ“Š Total samples: {len(results)}")
    print(f"ğŸ“ˆ Average F1 score: {avg_f1:.4f}")
    print(f"ğŸ¯ Exact Match rate: {avg_em:.4f}")
    print(f"ğŸ’¾ Detailed results saved to: {output_csv}")

if __name__ == "__main__":
    evaluate_jsonl("QA result.jsonl")
