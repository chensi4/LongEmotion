# ======================================
# Emotion Classification for Long Texts (GPUåŠ é€Ÿç‰ˆ)
# Compatible with PyCharm
# ======================================

import os
import torch
import pandas as pd
import numpy as np
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
import evaluate

# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"âœ… æ£€æµ‹åˆ°å¯ç”¨GPU: {torch.cuda.get_device_name(0)}")
    print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")
else:
    raise RuntimeError("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œè¯·æ£€æŸ¥CUDAé…ç½®")

# ==============================
# Step 1. æ•°æ®å¯¼å…¥ä¸æ¸…æ´—
# ==============================

data_path = r"emotion_data.csv"  # è¯·ä¿®æ”¹ä¸ºä½ çš„æ•°æ®è·¯å¾„

if not os.path.exists(data_path):
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {data_path}")

# è¯»å–å¹¶æ¸…æ´—æ•°æ®
df = pd.read_csv(data_path).reset_index(drop=True)
print("âœ… æˆåŠŸè¯»å–æ•°æ®ï¼Œå…± {} æ¡æ ·æœ¬".format(len(df)))

# æ•°æ®æ¸…æ´—
df = df[['text', 'label']].dropna()
valid_labels = ['positive', 'negative']
df = df[df['label'].isin(valid_labels)]
print("âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼Œå‰©ä½™ {} æ¡æœ‰æ•ˆæ ·æœ¬".format(len(df)))

# æ ‡ç­¾æ•´æ•°åŒ–
label2id = {'negative': 0, 'positive': 1}
df['label'] = df['label'].map(label2id)
id2label = {v: k for k, v in label2id.items()}
print("\nLabel mapping (strâ†’int):", label2id)
print(df.head())

# è½¬æ¢ä¸ºDatasetå¹¶åˆ é™¤ç´¢å¼•åˆ—
dataset = Dataset.from_pandas(df).remove_columns(["__index_level_0__"])
print(f"\nâœ… Datasetè½¬æ¢å®Œæˆï¼ŒåŒ…å«åˆ—ï¼š{dataset.column_names}")

# ==============================
# Step 2. æ¨¡å‹ä¸åˆ†è¯å™¨åŠ è½½ï¼ˆè‡ªåŠ¨åŠ è½½åˆ°GPUï¼‰
# ==============================
model_name = "allenai/longformer-base-4096"
print("\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨...")

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    padding_side="right"
)

# åŠ è½½æ¨¡å‹å¹¶è‡ªåŠ¨ç§»åŠ¨åˆ°GPU
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
).to(device)  # å…³é”®ï¼šå°†æ¨¡å‹ç§»åŠ¨åˆ°GPU


# ==============================
# Step 3. æ•°æ®é¢„å¤„ç†
# ==============================
def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=1024,  # GPUå¯æ”¯æŒæ›´é•¿æ–‡æœ¬ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
        return_overflowing_tokens=False
    )


print("\nğŸ§© æ­£åœ¨å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ...")
encoded_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=["text"]
)
encoded_dataset = encoded_dataset.train_test_split(test_size=0.2, seed=42)
print("âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼š")
print(f"   - è®­ç»ƒé›†ï¼š{len(encoded_dataset['train'])} æ¡")
print(f"   - æµ‹è¯•é›†ï¼š{len(encoded_dataset['test'])} æ¡")

# æ•°æ®æ•´ç†å™¨ï¼ˆGPUä¼˜åŒ–ç‰ˆï¼‰
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# ==============================
# Step 4. è¯„ä¼°æŒ‡æ ‡å®šä¹‰
# ==============================
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1_macro": f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    }


# ==============================
# Step 5. è®­ç»ƒå‚æ•°é…ç½®ï¼ˆGPUä¼˜åŒ–ï¼‰
# ==============================
training_args = TrainingArguments(
    output_dir="./emotion_results_gpu",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,  # GPUå¯å¢å¤§æ‰¹æ¬¡ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
    per_device_eval_batch_size=8,
    num_train_epochs=3,  # GPUå¯å¢åŠ è½®æ¬¡æå‡æ•ˆæœ
    weight_decay=0.01,
    logging_dir='./logs_gpu',
    logging_steps=200,
    load_best_model_at_end=True,
    report_to="none",
    remove_unused_columns=False,
    # GPUç›¸å…³é…ç½®
    fp16=True,  # æ··åˆç²¾åº¦è®­ç»ƒï¼ŒåŠ é€Ÿä¸”çœæ˜¾å­˜
    dataloader_pin_memory=True,  # æ•°æ®åŠ è½½åˆ°GPUæ—¶å¯ç”¨pin_memoryåŠ é€Ÿ
    gradient_accumulation_steps=1  # æ˜¾å­˜è¶³å¤Ÿæ—¶è®¾ä¸º1
)

# ==============================
# Step 6. å¯åŠ¨GPUè®­ç»ƒ
# ==============================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
    compute_metrics=compute_metrics,
    data_collator=data_collator
)

print("\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼ˆGPUæ¨¡å¼ï¼‰...\n")
trainer.train()

# ==============================
# Step 7. æ¨¡å‹è¯„ä¼°
# ==============================
print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
metrics = trainer.evaluate()
print(f"   - å‡†ç¡®ç‡(Accuracy)ï¼š{metrics['eval_accuracy']:.4f}")
print(f"   - å®F1åˆ†æ•°(F1-Macro)ï¼š{metrics['eval_f1_macro']:.4f}")

# ==============================
# Step 8. ä¿å­˜æ¨¡å‹
# ==============================
model_save_path = "./emotion_model_gpu"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{os.path.abspath(model_save_path)}")


# ==============================
# Step 9. GPUæ¨ç†é¢„æµ‹
# ==============================
def predict_emotion(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=1024,
        padding=True
    ).to(device)  # è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPU

    with torch.no_grad():
        outputs = model(**inputs)

    pred_id = torch.argmax(outputs.logits, dim=-1).item()
    return id2label[pred_id]


# æµ‹è¯•é¢„æµ‹
test_texts = [
    "The service was great and the food tasted delicious. I will come back again.",
    "I waited for 2 hours but no one responded. This is the worst experience ever."
]
print("\nğŸ§  é¢„æµ‹æµ‹è¯•ï¼š")
for i, text in enumerate(test_texts, 1):
    pred = predict_emotion(text)
    print(f"   æ–‡æœ¬{i}ï¼š{text[:60]}... â†’ é¢„æµ‹æƒ…ç»ªï¼š{pred}")
