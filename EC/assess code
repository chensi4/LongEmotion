# æ¨¡å‹è¯„ä¼°ä»£ç ï¼ˆé€‚é…4ç±»æƒ…ç»ªæ ‡ç­¾ï¼šhappy/sad/neutral/angryï¼‰
# ä¾èµ–åº“ï¼špandas, numpy, evaluate, sklearn, matplotlib
# ======================================

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import evaluate

# ------------------------------
# 1. é…ç½®å‚æ•°ï¼ˆéœ€æ ¹æ®ä½ çš„æ–‡ä»¶è·¯å¾„ä¿®æ”¹ï¼‰
# ------------------------------
PREDICTION_PATH = r"Emotion_Classification_Result (1).jsonl.txt"  # è®°äº‹æœ¬ä¸­çš„é¢„æµ‹ç»“æœè·¯å¾„
TEST_DATA_PATH = r"emotion_test_data.csv"  # æµ‹è¯•é›†è·¯å¾„ï¼ˆéœ€å«idã€true_labelåˆ—ï¼‰
LABEL_MAPPING = {  # æ ‡ç­¾æ˜ å°„ï¼šæ–‡æœ¬æ ‡ç­¾â†’æ•´æ•°ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    "happy": 0,
    "sad": 1,
    "neutral": 2,
    "angry": 3
}
ID2LABEL = {v: k for k, v in LABEL_MAPPING.items()}  # æ•´æ•°â†’æ–‡æœ¬æ ‡ç­¾ï¼ˆç”¨äºç»“æœå±•ç¤ºï¼‰


# ------------------------------
# 2. åŠ è½½é¢„æµ‹ç»“æœï¼ˆä»jsonlæ–‡ä»¶ï¼‰
# ------------------------------
def load_predictions(pred_path):
    """åŠ è½½é¢„æµ‹ç»“æœï¼Œè¿”å›DataFrameï¼ˆå«idã€predicted_emotionï¼‰"""
    if not os.path.exists(pred_path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°é¢„æµ‹ç»“æœæ–‡ä»¶ï¼š{pred_path}")
    
    predictions = []
    with open(pred_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:  # è·³è¿‡ç©ºè¡Œ
                pred_dict = json.loads(line)
                predictions.append(pred_dict)
    
    # è½¬ä¸ºDataFrameå¹¶æ£€æŸ¥æ ‡ç­¾åˆæ³•æ€§
    pred_df = pd.DataFrame(predictions)
    invalid_labels = pred_df[~pred_df["predicted_emotion"].isin(LABEL_MAPPING.keys())]
    if not invalid_labels.empty:
        raise ValueError(f"âŒ é¢„æµ‹ç»“æœä¸­å­˜åœ¨æ— æ•ˆæ ‡ç­¾ï¼š{invalid_labels['predicted_emotion'].unique()}")
    
    print(f"âœ… æˆåŠŸåŠ è½½é¢„æµ‹ç»“æœï¼šå…± {len(pred_df)} æ¡è®°å½•")
    return pred_df


# ------------------------------
# 3. åŠ è½½æµ‹è¯•é›†çœŸå®æ ‡ç­¾
# ------------------------------
def load_test_true_labels(test_path):
    """åŠ è½½æµ‹è¯•é›†çœŸå®æ ‡ç­¾ï¼Œè¿”å›DataFrameï¼ˆå«idã€true_labelï¼‰"""
    if not os.path.exists(test_path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•é›†æ–‡ä»¶ï¼š{test_path}")
    
    # è¯»å–æµ‹è¯•é›†ï¼ˆå‡è®¾åˆ—åä¸ºidã€true_labelï¼Œè‹¥ä¸åŒéœ€ä¿®æ”¹ï¼‰
    test_df = pd.read_csv(test_path, usecols=["id", "true_label"]).dropna()
    
    # æ£€æŸ¥æ ‡ç­¾åˆæ³•æ€§
    invalid_labels = test_df[~test_df["true_label"].isin(LABEL_MAPPING.keys())]
    if not invalid_labels.empty:
        raise ValueError(f"âŒ æµ‹è¯•é›†çœŸå®æ ‡ç­¾ä¸­å­˜åœ¨æ— æ•ˆæ ‡ç­¾ï¼š{invalid_labels['true_label'].unique()}")
    
    print(f"âœ… æˆåŠŸåŠ è½½æµ‹è¯•é›†çœŸå®æ ‡ç­¾ï¼šå…± {len(test_df)} æ¡è®°å½•")
    return test_df


# ------------------------------
# 4. å¯¹é½çœŸå®æ ‡ç­¾ä¸é¢„æµ‹æ ‡ç­¾
# ------------------------------
def align_true_and_pred(true_df, pred_df):
    """æŒ‰idå¯¹é½çœŸå®æ ‡ç­¾ä¸é¢„æµ‹æ ‡ç­¾ï¼Œè¿”å›æ•´æ•°ç¼–ç åçš„y_trueå’Œy_pred"""
    # æŒ‰idåˆå¹¶æ•°æ®ï¼ˆç¡®ä¿ä¸€ä¸€å¯¹åº”ï¼‰
    merged_df = pd.merge(
        true_df, 
        pred_df, 
        on="id", 
        how="inner",  # åªä¿ç•™ä¸¤è¾¹éƒ½å­˜åœ¨çš„idï¼ˆé¿å…ç¼ºå¤±ï¼‰
        validate="one_to_one"  # ç¡®ä¿æ¯ä¸ªidåªå¯¹åº”ä¸€æ¡è®°å½•
    )
    
    if len(merged_df) != len(true_df) or len(merged_df) != len(pred_df):
        missing_ids_true = set(true_df["id"]) - set(merged_df["id"])
        missing_ids_pred = set(pred_df["id"]) - set(merged_df["id"])
        print(f"âš ï¸  è­¦å‘Šï¼šéƒ¨åˆ†idæœªåŒ¹é…")
        if missing_ids_true:
            print(f"   - æµ‹è¯•é›†å­˜åœ¨ä½†é¢„æµ‹ç»“æœç¼ºå¤±çš„idæ•°é‡ï¼š{len(missing_ids_true)}")
        if missing_ids_pred:
            print(f"   - é¢„æµ‹ç»“æœå­˜åœ¨ä½†æµ‹è¯•é›†ç¼ºå¤±çš„idæ•°é‡ï¼š{len(missing_ids_pred)}")
    
    print(f"âœ… æœ€ç»ˆå¯¹é½çš„æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(merged_df)}")
    
    # å°†æ–‡æœ¬æ ‡ç­¾è½¬ä¸ºæ•´æ•°ï¼ˆç”¨äºè®¡ç®—æŒ‡æ ‡ï¼‰
    y_true = merged_df["true_label"].map(LABEL_MAPPING).values
    y_pred = merged_df["predicted_emotion"].map(LABEL_MAPPING).values
    
    return y_true, y_pred, merged_df


# ------------------------------
# 5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
# ------------------------------
def compute_evaluation_metrics(y_true, y_pred):
    """è®¡ç®—å¤šåˆ†ç±»è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€å®F1ã€å¾®F1ã€å„ç±»åˆ«F1"""
    # åŠ è½½è¯„ä¼°æŒ‡æ ‡ï¼ˆä¸åŸè®­ç»ƒä»£ç ä¸€è‡´ä½¿ç”¨evaluateåº“ï¼‰
    accuracy_metric = evaluate.load("accuracy")
    f1_metric = evaluate.load("f1")
    
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    accuracy = accuracy_metric.compute(predictions=y_pred, references=y_true)["accuracy"]
    f1_macro = f1_metric.compute(predictions=y_pred, references=y_true, average="macro")["f1"]
    f1_micro = f1_metric.compute(predictions=y_pred, references=y_true, average="micro")["f1"]
    
    # è®¡ç®—å„ç±»åˆ«å•ç‹¬F1
    per_class_f1 = f1_metric.compute(predictions=y_pred, references=y_true, average=None)["f1"]
    per_class_f1_dict = {ID2LABEL[i]: round(f1, 4) for i, f1 in enumerate(per_class_f1)}
    
    # æ•´ç†ç»“æœ
    metrics = {
        "å‡†ç¡®ç‡(Accuracy)": round(accuracy, 4),
        "å®F1(Macro-F1)": round(f1_macro, 4),
        "å¾®F1(Micro-F1)": round(f1_micro, 4),
        "å„ç±»åˆ«F1(Per-Class F1)": per_class_f1_dict
    }
    
    return metrics


# ------------------------------
# 6. ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆå¯è§†åŒ–æ¨¡å‹è¡¨ç°ï¼‰
# ------------------------------
def plot_confusion_matrix(y_true, y_pred, save_path="./confusion_matrix.png"):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶ä¿å­˜"""
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    labels = [ID2LABEL[i] for i in range(len(LABEL_MAPPING))]
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.rcParams['font.sans-serif'] = ['SimHei']  # æ”¯æŒä¸­æ–‡
    plt.figure(figsize=(8, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)
    plt.title("æƒ…ç»ªåˆ†ç±»æ¨¡å‹æ··æ·†çŸ©é˜µ", fontsize=14)
    plt.ylabel("çœŸå®æ ‡ç­¾", fontsize=12)
    plt.xlabel("é¢„æµ‹æ ‡ç­¾", fontsize=12)
    plt.tight_layout()  # è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾è¢«æˆªæ–­
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ï¼š{os.path.abspath(save_path)}")


# ------------------------------
# 7. ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´è¯„ä¼°æµç¨‹
# ------------------------------
def main():
    print("=" * 50)
    print("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°æµç¨‹")
    print("=" * 50)
    
    # æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
    pred_df = load_predictions(PREDICTION_PATH)
    test_df = load_test_true_labels(TEST_DATA_PATH)
    
    # æ­¥éª¤2ï¼šå¯¹é½æ ‡ç­¾
    y_true, y_pred, merged_df = align_true_and_pred(test_df, pred_df)
    
    # æ­¥éª¤3ï¼šè®¡ç®—æŒ‡æ ‡
    metrics = compute_evaluation_metrics(y_true, y_pred)
    
    # æ­¥éª¤4ï¼šè¾“å‡ºç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“‹ æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("=" * 50)
    for key, value in metrics.items():
        if isinstance(value, dict):
            print(f"\n{key}ï¼š")
            for label, f1 in value.items():
                print(f"   - {label}: {f1}")
        else:
            print(f"{key}ï¼š{value}")
    
    # æ­¥éª¤5ï¼šç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(y_true, y_pred)
    
    print("\n" + "=" * 50)
    print("âœ… è¯„ä¼°æµç¨‹å®Œæˆï¼")
    print("=" * 50)


if __name__ == "__main__":
    main()
